{
  "articles": [
    {
      "path": "human_prosody_scoring.html",
      "title": "Human-Rated Prosody Study",
      "author": [
        {
          "name": "Joseph F. T. Nese",
          "url": {}
        },
        {
          "name": "Makayla Whitney",
          "url": {}
        },
        {
          "name": "Julie Alonzo",
          "url": {}
        },
        {
          "name": "Leilani Sáez",
          "url": {}
        },
        {
          "name": "Rhonda N. T. Nese",
          "url": {}
        }
      ],
      "date": "January 2020",
      "contents": "\r\n\r\nContents\r\nPurpose\r\nAudio Recordings\r\nCORE ORF Passages\r\nAudio File Selection\r\n\r\nResearch Team\r\nProsody Rubric Development\r\nThe CORE + Prosody Rubric\r\nExemplar Audio Files\r\n\r\nProsody Rater Recruitment\r\nProsody Certification\r\nTraining Development & Implementation\r\nTraining Session #1\r\nThe Task\r\n\r\nTraining Session #2\r\n\r\nProsody Rating Procedures\r\nProsody Raters: Sample\r\nHighest Degree Earned\r\nProfessional Roles\r\nExperience\r\n\r\nProsody Raters: Certification\r\nProsody Ratings\r\nAcknowledgments\r\nAuthor Contributions\r\n\r\n\r\n\r\nPurpose\r\nOral reading fluency (ORF), generally defined as reading quickly, accurately, and with prosody, is an essential part of reading proficiency. Prosody, reading with appropriate expression and phrasing, is one way to demonstrate that a reader understands the meaning of the text.\r\nThe purpose of this study is to collect prosody ratings of audio recordings of students’ ORF. These human-rated prosody scores will serve as the basis for training an algorithm that can be used to automatically generate prosody scores from students’ oral reading.\r\nAudio Recordings\r\nAudio recordings of students in Grades 2 through 4 reading brief ORF passages were collected as part of an IES funded project called Computerized Oral Reading Evaluation, or CORE. CORE combines automatic speech recognition (ASR) to score ORF accuracy and rate, with a latent variable psychometric model to scale, equate, and link scores across Grades 2 through 4. The primary goal of CORE is to develop an ORF assessment system with the potential to reduce: (a) human ORF administration errors, by standardizing administration setting, delivery, and scoring; (b) the time cost of ORF administration, by allowing small-group or whole-classroom testing; (c) the resource cost to train staff to administer and score the ORF assessment; and (d) the standard error of ORF measurement.\r\nThe work conducted in the current project extends this line of research by incorporating prosody into the measurement model.\r\nThe Consequential Validity Study from the original CORE project conducted in 2017-18 and 2018-19 resulted in the accumulation of 90,720 audio files. Of these, 8,713 were excluded from the current study because they were recordings of students reading the criterion easyCBM ORF passages from the original study while the remaining 82,007 (90.4%) represented recordings of students reading brief (approximately 50-85 word) passages developed specifically for the CORE project. From the 82,007 eligible audio recordings, only those that were at least ten seconds long were selected (to screen for empty or incomplete files) for a final corpus of 78,712 audio files.\r\nCORE ORF Passages\r\nCORE passages were written by a former teacher, who also co-wrote the original easyCBM ORF and reading comprehension passages. Each CORE passage is an original work of fiction, and within 5 words of a targeted length: long = 85 words or medium = 50 words. Each passage has a beginning, middle, and end, follows either a “problem/resolution” or “sequence of events” format, and contains minimal use of dialogue and symbols. Exclusion rules for what could not appear in passages included: religious themes; trademark names, places, products; cultural/ethnic depictions; age-inappropriate themes (e.g., violence, guns, tobacco, drugs). All final CORE passages were reviewed by two experts in assessment for screening and progress monitoring for errors (e.g., format and grammatical), and bias (e.g., gender, cultural, religious, geographical). Final passages included 150 total passages, 50 at each of Grades 2-4, with 20 long passages (80-90 words), and 30 medium passages (45-55 words) for each grade.\r\nAudio File Selection\r\nFor the current study, a two-step process was used to select 200 audio files for 10 CORE ORF passages at each of Grades 2 through 4.\r\nFirst, for each grade and passage length the 5 CORE passages with the greatest number of audio file records were selected to create as large an item bank as possible. This process resulted in the selection of 10 CORE passages (5 long and 5 medium) for each of Grades 2 – 4, 30 passages in all.\r\n\r\n\r\n\r\nSecond, stratified random sampling was applied to select 200 audio recordings of each CORE passage, oversampling for English learners (ELs) and students with disabilities (SWDs), two student groups for which the ASR may be less accurate. The stratified random sampling plan led to the following quantities of sampled audio files: 5 students (2.5%) dually classified as EL and SWD, 65 students (32.5%) classified as EL only, 65 students (32.5%) classified as SWD only, and 65 students (32.5%) classified as neither EL nor SWD. A cascading logic was implemented, such that when fewer than 5 recordings included students dually classified as EL and SWD, the remainder of recordings was sampled from students classified as EL only. If there were insufficient audio recordings from EL only students, the remainder was sampled from students classified as SWD only. The remainder of audio recordings was sampled from students classified as neither EL nor SWD, of which there were ample recordings.\r\nThe design of the project stipulated that each of the 200 audio files per CORE passage (10 passages * 3 grade levels * 200 recordings = 6,000 audio files) was to be rated for prosody by two different raters for a total of 12,000 prosody ratings (6,000 * 2 ratings = 12,000 total prosody ratings). The 6,000 audio files were grouped into 120 sets of 50 for distribution to human raters. The 200 audio files per CORE passage were split into four sets, such that each set of 50 contained audio files of students reading the same passage. This structure was used to allow raters to get familiar with a passage and thus provide more reliable ratings. The sets were manually distributed to raters required, descending by grade and passage such that all four sets of the first Grade 4 passage were sent to the first eight raters (as each set was rated twice), and continuing through the last Grade 2 passage.\r\n\r\n\r\n\r\nOf the 6,000 selected audio files 836 (14%) had to be replaced because they had no audio available to score; either there was no audio (e.g., the student was muted or advanced without reading), or the audio did not allow the rater to confidently give a prosody score (e.g., poor audio quality, too much background noise, a very quiet reader). All audio files were replaced with a reading from the same CORE passage. For n audio files that needed to be replaced for a CORE passage, n \\(\\times\\) 1.175 (17.5% of n) were sampled to account for potential audio recording with no available audio in the replacement set. An effort was made to replace audio files read by a student with the same EL/SWD classification. That is, the same cascading logic as previously described was applied, such that when the number of recordings for students dually classified as EL and SWD was less than required in our sampling plan, the remainder was sampled from students classified as EL only. If there were insufficient audio recordings from EL only students, the remainder was sampled from students classified as SWD only. Insufficient recordings led to the remainder of audio recordings being sampled from students classified as neither EL nor SWD, of which there were ample recordings. An additional 998 audio files were distributed to the human raters as replacements.\r\nAfter the 998 audio file replacements were scored, there remained five CORE passages that had less than 200 audio files with two different prosody ratings: three CORE passages had 199 audio files, and two had 197 audio files. For n (1 or 3) audio files that needed to be replaced for a CORE passage, n \\(\\times\\) 7 were sampled to account for potential audio recording with no available audio in the replacement set. These audio files were randomly sampled (without stratifying for ELs and SWDs) from those remaining for the respective CORE passages.\r\nAfter all usable audio files were selected the full sample included 6,096 audio files each rated twice. Of these from 1,342 students (4,068 in Grade 2, 4,070 in Grade 3, 4,054 in Grade 4). The number of audio files per student in the final sample ranged from 2 to 44.\r\n\r\n\r\n\r\nThe results of the stratification yielded a sample of 6,096 audio files that was 2% (n = 256) EL and SWD, 23% (n = 2848) EL only, 29% (n = 3532) SWD only, and 46% (n = 5556) neither EL or SWD.\r\n\r\n\r\n\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#kpkrexptmw .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 12px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_title {\r\n  color: #333333;\r\n  font-size: small;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#kpkrexptmw .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 6px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#kpkrexptmw .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#kpkrexptmw .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#kpkrexptmw .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#kpkrexptmw .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#kpkrexptmw .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 5px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#kpkrexptmw .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#kpkrexptmw .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#kpkrexptmw .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#kpkrexptmw .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#kpkrexptmw .gt_row {\r\n  padding-top: 3px;\r\n  padding-bottom: 3px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#kpkrexptmw .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#kpkrexptmw .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#kpkrexptmw .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#kpkrexptmw .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#kpkrexptmw .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#kpkrexptmw .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#kpkrexptmw .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#kpkrexptmw .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#kpkrexptmw .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#kpkrexptmw .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#kpkrexptmw .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#kpkrexptmw .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#kpkrexptmw .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#kpkrexptmw .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#kpkrexptmw .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-weight: normal;\r\n  font-size: 65%;\r\n}\r\nSample Demographic Characteristics\r\n    Characteristic\r\n      \r\n        By Student\r\n      \r\n      \r\n        By Audiofile\r\n      \r\n    N = 1,3421\r\n      N = 6,0961\r\n    Grade\r\n\r\n2\r\n464 (35%)\r\n2,034 (33%)3\r\n430 (32%)\r\n2,035 (33%)4\r\n448 (33%)\r\n2,027 (33%)Gender\r\n\r\nFemale\r\n595 (49%)\r\n2,488 (46%)Male\r\n609 (51%)\r\n2,919 (54%)(Missing)\r\n138\r\n689Ethnicity\r\n\r\nHispanic/Latino\r\n315 (26%)\r\n1,750 (32%)Not Hispanic/Latino\r\n889 (74%)\r\n3,657 (68%)(Missing)\r\n138\r\n689Race\r\n\r\nAmerican Indian/Native Alaskan\r\n49 (4.1%)\r\n275 (5.1%)Asian\r\n9 (0.7%)\r\n64 (1.2%)Black/African American\r\n6 (0.5%)\r\n27 (0.5%)Hispanic\r\n46 (3.8%)\r\n172 (3.2%)Multi-Racial\r\n107 (8.9%)\r\n470 (8.7%)Native Hawaiian/Other Pacific Islander\r\n4 (0.3%)\r\n20 (0.4%)White\r\n983 (82%)\r\n4,379 (81%)(Missing)\r\n138\r\n689Students with Disabilities (SWD)\r\n229 (17%)\r\n1,894 (31%)English Language Learners (EL)\r\n188 (14%)\r\n1,552 (25%)Stratification Groups\r\n\r\nEL & SWD\r\n23 (1.7%)\r\n128 (2.1%)EL only\r\n165 (12%)\r\n1,424 (23%)Not EL or SWD\r\n948 (71%)\r\n2,778 (46%)SWD only\r\n206 (15%)\r\n1,766 (29%)\r\n        \r\n          1\r\n          \r\n           \r\n          n (%)\r\n          \r\n      \r\n    \r\n\r\n(Back to Table of Contents)\r\nResearch Team\r\nThe research team comprised four faculty with expertise in the assessment of students’ reading fluency (specializations included: two doctorates in School Psychology, one doctorate in Educational Leadership with a specialization in Learning Assessment/Systems Performance, and one doctorate in Educational Psychology), and one graduate research assistant with experience in literacy. The research team met weekly from August through November 2020, to refine a prosody scoring rubric, score audio files to be used as training and demonstration exemplars, and develop two online sessions to train prosody raters. These sessions were delivered live as well as recorded for asynchronous delivery for raters who were unable to attend in person.\r\n(Back to Table of Contents)\r\nProsody Rubric Development\r\nThe research team began with the prosody scoring rubric developed by the National Assessment of Educational Progress (NAEP; 1), a four-point scale (below) that focuses on phrasing, adherence to the author’s syntax, and expressiveness to assess prosody at Grade 4.\r\n\r\n\r\n\r\nFigure 1: From Daane, Campbell, Grigg, Goodman, & Oranje (2005)\r\n\r\n\r\n\r\nAlthough NAEP only applied the scoring rubric to Grade 4, our research team made the decision to use the rubric across Grades 2 through 4, independent of grade and based on the absolute prosody criteria specified for each of the four prosody levels.\r\nTo help draw clear differences between the four prosody levels across grades, parts of the Multi-Dimensional Fluency Scoring Guide (MFSG; 2) were incorporated into the original NAEP rubric.\r\n\r\n\r\n\r\nFigure 2: From Rasinski, Rikli, & Johnston (2009)\r\n\r\n\r\n\r\nThe MFSG focuses on assessing aspects of expression, phrasing, smoothness, pacing, and accuracy. The research team expanded and refined the NAEP prosody rubric with select parts of the MFSG to add more specific language and examples.\r\nA systematic process for adapting the NAEP rubric was conducted in August and September, 2020. First, 30 audio recordings were dispersed among the research team and scored individually by the four faculty. These scores and commentary were documented, analyzed, and discussed during the following week’s meetings. A summary of the team’s individual scores was presented, highlighting areas of agreement and disagreement: 9 audio files (30%) received the same score across all four raters; 13 (43%) received the same score across three raters with the fourth rating different by one prosody level; 4 (13%) were split down the middle, with two sets of identical scores that differed by one prosody level; and 4 (13%) received three different prosody scores, two of which were scored the same and two of which differed by two prosody levels. Based on inconsistent variation within the team, it was decided that more in-depth explanation was needed for each of the score levels.\r\nTo achieve this goal, the team listened to recordings together during online meetings and iteratively specified deeper distinctions between adjacent scores using the MFSG factors of pace, phrasing, and expression and volume. The 30 audio recordings were again scored individually by the four faculty: 12 (40%) audio files (30%) received the same score across all four raters; 12 (40%) received the same score across three raters, with the fourth rating different by one prosody level; and 6 (20%) were split down the middle, with two sets of identical scores that differed by one prosody level.\r\nThe team further refined the adapted rubric to clarify rating criteria and arrive at more unequivocal prosody scores. That is, the first version of the adapted rubric did not address whether the overall storyline was “represented” by the reader. After working through various examples, the research team added the following distinctions for each proficiency level (italic text represents additions from the MFSG, and regular text represents additions made by the research team).\r\nLevel 1: Reads slowly and laboriously. Story line is incoherent.\r\nLevel 2: Reads moderately slowly. Overall meaning of the text is preserved.\r\nLevel 3: Reads with a mixture of run-ons, mid-sentence pauses for breath, and some choppiness. There is reasonable stress and intonation.\r\nLevel 4: Reads smoothly with some breaks, but self-corrects with difficult words and/or sentence structures. Reads with varied volume and expression (like talking to a friend with voice matching the interpretation of the passage).\r\nThe CORE + Prosody Rubric\r\n\r\n\r\n\r\nExemplar Audio Files\r\nAfter finalizing the refined rubric, the research team came to unanimous agreement on the 30 audio files. Then, additional audio files were sought with the goal of having 15 exemplar audio files for each of the four prosody levels. ORF data from the CORE project were used to find (de-identified) students whose fall easyCBM ORF scores clustered around a specified percentile; for example, students who scored at or below the 20th percentile as potential candidates for prosody scores of Levels 1 or 2, and students who scored above the 90th percentile as potential candidates for a prosody score of Level 4. Using this process, the team identified an additional 31 audio files, each of which were independently scored by two of the five research team personnel. Of these, 21 (68%) received the same score across the two raters. The remaining 10 audio files were scored by a third member of the research team, and discussed by the full research team until unanimous score agreement was achieved. Additional exemplar audio files were still needed for Levels 2 and 4, so 16 additional files were identified and underwent the same process just described.\r\nIn total, 81 passages were identified and scored by the research team as exemplars for trainings and demonstrations: 20 at Level 4, 23 at Level 3, 15 at Level 2, and 23 at Level 1. Of these, 24 were used for Training, 25 were used for Certification (both described below), and the remaining 32 were retained in case of future need.\r\nHuman prosody raters were recruited and required to complete two Training Sessions, and meet Prosody Certification criterion.\r\n(Back to Table of Contents)\r\nProsody Rater Recruitment\r\nEducators (teachers and specialized professionals) were targeted as potential prosody raters. Potential prosody raters were recruited in October – November 2020 from two sources: teacher participants from the original CORE project, and through an announcement placed on the easyCBM - Lite and Deluxe sites for three weeks (10/19/2020 – 11/6/2020). These two easyCBM sites have over 79,000 registered users.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nApproximately 300 people responded to the announcement posted on the easyCBM sites. These respondents were then sent an email introducing them to the project, the task required of them, what they could expect (payment terms, remote work, and work commitment), training requirements, the prosody certification process, and next steps (a Qualtrics Registration form requiring demographic information, teaching experience, and a W9).  The PI (J. F. T. Nese) corresponded with all potential prosody raters throughout the process.\r\n\r\n\r\n\r\nOf the 300 respondents, 119 completed the Registration form, and 78 completed the required trainings. (No information is available as to why some chose not to complete the Registration or the trainings.)\r\nProsody Certification\r\nPrior to starting work, each prosody rater was required to complete the Training and demonstrate scoring proficiency by obtaining 80% or higher agreement with the research team’s pre-determined rating on two different sets of five audio files. Raters had five opportunities to achieve at least 80% (4/5) on two of the prosody assessments. Raters unable to achieve two passing scores received payment for their participation in training ($45) but were not eligible to continue their participation in this project.\r\nAll Prosody Certification Assessments were delivered with Google Forms’ Quiz feature. All participants took the first Prosody Certification after Training #1 and before Training #2. The remaining Prosody Certification assessments were taken at each participant’s pace. Once a participant met the prosody certification criteria by scoring at least 80% on two assessments, they began scoring audio files (and took no more assessments).\r\nOf the 78 people who completed the Trainings, 63 (81%) met prosody certification, 2 (3%) failed to meet certification, and 13 (17%) did not complete the certification process.\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#odgwcftxbr .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#odgwcftxbr .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 6px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#odgwcftxbr .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#odgwcftxbr .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#odgwcftxbr .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#odgwcftxbr .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#odgwcftxbr .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 5px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#odgwcftxbr .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#odgwcftxbr .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#odgwcftxbr .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#odgwcftxbr .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#odgwcftxbr .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#odgwcftxbr .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#odgwcftxbr .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#odgwcftxbr .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#odgwcftxbr .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#odgwcftxbr .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#odgwcftxbr .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#odgwcftxbr .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#odgwcftxbr .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#odgwcftxbr .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#odgwcftxbr .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#odgwcftxbr .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#odgwcftxbr .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#odgwcftxbr .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#odgwcftxbr .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#odgwcftxbr .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-weight: normal;\r\n  font-size: 65%;\r\n}\r\nProsody Certification Assessment Passing Rates\r\n    \r\n      n\r\n      Fail\r\n      Fail (%)\r\n      Pass\r\n      Pass (%)\r\n    Certification #1\r\n78\r\n37\r\n47%\r\n41\r\n53%Certification #2\r\n73\r\n20\r\n27%\r\n53\r\n73%Certification #3\r\n41\r\n6\r\n15%\r\n35\r\n85%Certification #4\r\n10\r\n5\r\n50%\r\n5\r\n50%Certification #5\r\n4\r\n1\r\n25%\r\n3\r\n75%\r\n\r\nOf the 78 people who took Certification #1, 53% passed by scoring 4 or 5; 73% of the 73 people who took Certification #2 passed; 85% of the 41 people who took Certification #3 passed; 50% of the 10 people who took Certification #4 passed; and 75% of the 4 people who took Certification #5 passed.\r\n\r\n\r\n\r\nTraining Development & Implementation\r\nA two-session training for prosody raters was developed for in-person, online delivery across two meetings in November 2020. Each Training Session was delivered twice: on Friday and the subsequent Monday afternoon (after the school day had concluded), and participants could attend either the Friday or the Monday training. There was one week between Training Session #1 and Session #2. For Training Session #1, Day 1 was held on 11/13/2020 and Day 2 was held on 11/16/2020. For Training Session #2, Day 1 was held on 11/20/2020 and Day 2 was held on 11/23/2020.\r\nThree members of the team were present to deliver content and answer questions using Powerpoint slides presented on the Zoom platform for web-based, live interaction. All trainings were recorded via Zoom for asynchronous training for participants who could not attend one or both of the live trainings. In total, 78 people completed the trainings.\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#alkkeuviro .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#alkkeuviro .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 6px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#alkkeuviro .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#alkkeuviro .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#alkkeuviro .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#alkkeuviro .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#alkkeuviro .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 5px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#alkkeuviro .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#alkkeuviro .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#alkkeuviro .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#alkkeuviro .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#alkkeuviro .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#alkkeuviro .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#alkkeuviro .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#alkkeuviro .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#alkkeuviro .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#alkkeuviro .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#alkkeuviro .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#alkkeuviro .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#alkkeuviro .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#alkkeuviro .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#alkkeuviro .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#alkkeuviro .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#alkkeuviro .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#alkkeuviro .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#alkkeuviro .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#alkkeuviro .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-weight: normal;\r\n  font-size: 65%;\r\n}\r\nTraining Sessions Attendance\r\n    Training Session\r\n      Day 1\r\n      Day 2\r\n      Asynchronous\r\n    Session #1\r\n35\r\n35\r\n8Session #2\r\n34\r\n35\r\n9\r\n\r\nThe research team created a website for participants to access training resources. The website included: the seven-step process for scoring audio files; the prosody rubric as a resource to print or keep open when rating; training materials, including the presentation slides, and a recording of each Training Session; the 24 exemplar audio files from Training Session #1; the link to the scoring site; and an About page with information about the study.\r\nTraining Session #1\r\nDuring Training Session #1 (2 hours), study logistics and key concepts were explained to potential raters. Training included: information about the project context; a comprehensive review of prosody; the task of rating audio recordings for prosody; an explanation of the rubric and how to rate recordings; how to earn certification as prosody rater; the expectations and payment structure; a set of 12 exemplar audio files (about one recording per prosody level for each of Grades 2 – 4); and a practice exercise, consisting of 12 exemplar audio files (three at each of the four prosody levels presented randomly) followed by a discussion of each and the qualities that made it a specific prosody level.\r\nParticipants were also introduced to prosody scoring in partial increments of 0.5 to facilitate prosody ratings in cases of nuanced uncertainty. For example, if a rater’s prosody rating was undecided between Level 2 and 3, they could score it as a 2.5. For the purposes of the study, all half scores (i.e., 1.5, 2.5, and 3.5) were rounded down because they did not meet the threshold for a higher score.\r\nThe Training Session #1 practice exercise involved three rounds of listening to each audio file. The purpose of the first listen was to pay attention to the passage’s general meaning so that raters would have a general sense of what the passage was about and the degree to which the student’s reading conveyed the meaning. The purpose of the second listen was to train raters to pay attention to reading style (i.e., word-by-word, awkward word groups, conversational) and to notice whether the author’s syntax was preserved. The third listen was used to train raters to pay attention to expressiveness. This step-by-step process was designed to train raters to attend to all aspects of the rubric, and not to focus exclusively on any single aspect. The team developed a seven-step guide for listening to and scoring audio recordings.\r\nThe Task\r\n\r\n\r\n\r\nAfter listening to example recordings to clarify the scale, participants were able to practice on their own. Recordings were played, and participants were asked to first think about how they would rate the recording without sharing their scores, and then they were prompted to type their prosody score for the recorded reading into the Zoom platform’s chat box feature. Participants’ reasoning for scores was discussed as a large group with research team members facilitating the discussion and using the rubric to emphasize points made.\r\nAfter Training Session #1, participants were given the first Prosody Certification assessment, which consisted of five audio files to be scored individually, on their own time, before Training Session #2.\r\nOf the 78 people who took the Prosody Certification Assessment #1, 41 (53%) passed and 37 (47%) did not. Note that Certification #1 was taken after Training Session #1, before the entire Training process was complete.\r\n\r\n\r\n\r\nTraining Session #2\r\nOne week later, participants again met with the research team for Training Session #2 (1.5 hours). Training Session #2 consisted primarily of a review of the five audio files from Prosody Certification #1. Participants were asked to listen to an audio file, with the prosody score provided by the training facilitator, and to identify key features that justified the score. After listening to an audio file, they were asked to share in the Zoom platform’s chat box prosody rubric features (shown on the screen) of the reading that corresponded to the score. The training facilitator read aloud and discussed the relevant and important prosody score features, using the rubric to confirm scores with participants. Participants were encouraged to ask questions if they did not understand or disagreed with the prosody score. Each audio file was played multiple times (three to six) to solidify the score and rationale for the attendees. This process was repeated for each of the five audio files.\r\nParticipants were then given an introduction to and demonstration of the software that they would use to score the audio files for prosody if they met certification. They were also introduced to the training website.\r\n(Back to Table of Contents)\r\nProsody Rating Procedures\r\nProsody Raters: Sample\r\n\r\n\r\n\r\nThe final prosody sample included 56 prosody raters, 7 from each of FL and IL, 4 from each of OR, 3 from each of IN, KS, NV and OH, 2 from each of GA, ID, KY, MI, NC and VA, and 1 from each of AL, AZ, CA, CO, LA, MT, NM, NY, PA, SC, TN, TX, UT and WA. Nearly all (54) raters were female, 1 was non-binary, and 1 chose not to response.\r\nHighest Degree Earned\r\n\r\n\r\n\r\nAmong the prosody raters, 32 (57%) earned a Masters in education, 18 (32%) earned a Bachelor, 2 (4%) earned a Masters in another field, 2 (4%) earned an Associate, and 2 (4%) earned a Doctorate.\r\n\r\n\r\n\r\nProfessional Roles\r\nThe professional roles of the raters were as follows:\r\n16 (29%) were special education teachers\r\n13 (23%) were general education teachers\r\n10 (18%) were reading/literacy specialists\r\n9 (16%) reported as “other”\r\n(i.e., dyslexia program specialist; literacy tutor/data analyst/testing coordinator; project consultant supporting literacy, behavior and MTSS; RTI coordinator/interventionist; RTI specialist; MTSS lead; EC compliance case manager; Master’s student; and undergraduate student studying elementary education)\r\n\r\n3 (5%) reported as “school psychologist | social worker | counselor | behavior specialist | etc.”\r\n2 (4%) reported as “administrator | principal | district support”\r\n2 (4%) were retired, and reported their role as special educators before retirement\r\n1 (2%) an “other content area specialist” (i.e., ESOL)\r\nExperience\r\n\r\n\r\n\r\nNearly all of the prosody raters (n = 47, 84%) worked at the elementary school level, 4 (7%) worked at the middle school level, 2 (4%) worked at the high school level, 1 (2%) worked at the elementary and middle levels, 1 (2%) worked at all three levels, and 1 (2%) worked with adults.\r\nThe average years experience as an educator was 14 years (SD = 9.9).\r\n\r\n\r\n\r\n(Back to Table of Contents)\r\nProsody Raters: Certification\r\nAll 56 prosody raters met the prosody certification criteria by scoring at least 80% on two Prosody Certification Assessments. Note that Certification #1 was taken after Training Session #1, before the Training was complete.\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#hrichdnlqo .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#hrichdnlqo .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 6px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#hrichdnlqo .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#hrichdnlqo .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#hrichdnlqo .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#hrichdnlqo .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#hrichdnlqo .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 5px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#hrichdnlqo .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#hrichdnlqo .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#hrichdnlqo .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#hrichdnlqo .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#hrichdnlqo .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#hrichdnlqo .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#hrichdnlqo .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#hrichdnlqo .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#hrichdnlqo .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#hrichdnlqo .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#hrichdnlqo .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#hrichdnlqo .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#hrichdnlqo .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#hrichdnlqo .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#hrichdnlqo .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#hrichdnlqo .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#hrichdnlqo .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#hrichdnlqo .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#hrichdnlqo .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#hrichdnlqo .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-weight: normal;\r\n  font-size: 65%;\r\n}\r\nProsody Certification Assessment Passing Rates\r\n    \r\n      n\r\n      Fail\r\n      Fail (%)\r\n      Pass\r\n      Pass (%)\r\n    Certification #1\r\n56\r\n25\r\n45%\r\n31\r\n55%Certification #2\r\n54\r\n11\r\n20%\r\n43\r\n80%Certification #3\r\n30\r\n2\r\n7%\r\n28\r\n93%Certification #4\r\n6\r\n2\r\n33%\r\n4\r\n67%Certification #5\r\n2\r\n0\r\n0%\r\n2\r\n100%\r\n\r\nOf the 56 people who took Certification #1, 55% passed by scoring 4 or 5; 80% of the 54 people who took Certification #2 passed; 93% of the 30 people who took Certification #3 passed; 67% of the 6 people who took Certification #4 passed; and 100% of the 2 people who took Certification #5 passed.\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe plot below shows which two Prosody Certification Assessments the raters met prosody certification. Twenty-five raters (45%) passed Certification #1 and Certification #2, 17 (30%) passed Certification #2 and Certification #3, 6 (11%) passed Certification #1 and Certification #3, 4 (7%) passed Certification #3 and Certification #4, 1 (2%) passed Certification #2 and Certification #5, and 1 (2%) passed Certification #3 and Certification #5.\r\n\r\n\r\n\r\n(Back to Table of Contents)\r\nProsody Ratings\r\nThe 56certified prosody raters created a profile on a project-designed Moodle site. Raters each created their own log-in information for the system. The system allowed raters to skip audio files, go back and change scores, and complete the rating in multiple sessions, stopping and re-starting as needed. In addition to the seven-point prosody scale (1, 1.5, 2, 2.5, 3, 3.5, 4), raters were also given an option to note “No audio available to score” in case there was no audio (e.g., the student was muted or advanced without reading) or the audio did not allow the rater to confidently give a score (e.g., poor audio quality, too much background noise, a very quiet reader).\r\nThe prosody raters were instructed to first complete a Prosody Review containing four exemplar files, one at each prosody level (i.e, 1, 2, 3, 4), prior to rating their first set of audio files. The expectation was set in training that raters must complete a minimum of 50 audio files; there was no maximum. Upon completion of the first set of recordings, raters emailed PI Nese to receive another set of recordings to rate.\r\nIn this manner, all 6,096 audio recordings were rated by two different prosody raters from November 28, 2020 through February 8, 2021, for a total of 12,192 human prosody scores.\r\nThe median number of audio files scored by prosody raters was 160, with a range from 38 to 747 (Mean = 218, SD = 177).\r\n\r\n\r\n\r\n(Back to Table of Contents)\r\nAcknowledgments\r\nThis is a place to recognize people and institutions. It may also be a good place to acknowledge and cite software that makes your work possible.\r\nAuthor Contributions\r\nWe strongly encourage you to include an author contributions statement briefly describing what each author did.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nDaane et al. (2005)↩︎\r\nRasinski et al. (2009)↩︎\r\n",
      "last_modified": "2022-02-14T11:49:49-08:00"
    },
    {
      "path": "index.html",
      "title": "CORE + Prosody",
      "author": [],
      "contents": "\r\nCORE + Prosody is a comprehensive measure of reading fluency that unites accuracy, rate, and prosody of students’ oral reading.\r\nOral reading fluency is an essential part of reading proficiency1, and is perhaps the most prevalent reading assessment used in classrooms across the U.S.; however, these traditional assessments measure only accuracy and rate, and entirely neglect prosody. Prosody is reading with appropriate expression and phrasing, and is one way a reader demonstrates they understand the meaning of the text. Research has shown prosody to be an important indicator of comprehension, beyond accuracy and rate alone, particularly among developing readers2.\r\nThe purpose of CORE + Prosody is to develop and validate an automated scoring system to measure, unite, and scale the accuracy, rate, and prosody in oral reading fluency to be used as a screening and progress monitoring measure for students in Grades 2 through 4.\r\nCORE + Prosody is a four-year project funded by the Institute of Education Sciences (IES), the statistics, research, and evaluation arm of the U.S. Department of Education.\r\n\r\n.panelset{--panel-tab-active-foreground: #c03728;--panel-tab-hover-foreground: #e68c7c;--panel-tab-font-family: Merriweather;}\r\n\r\n\r\nGoals\r\nCORE + Prosody Research Goals\r\nApply a machine learning model to measure and score reading prosody.\r\nDevelop a psychometric model to produce a comprehensive measure of fluency that includes prosody in addition to accuracy and rate.\r\nApply this psychometric model to a scaling framework by calibrating passage parameters on a common scale across grade levels (i.e., vertically linked), providing an advantage for across-grades growth monitoring of students oral reading fluency.\r\nAnalyze the relation between accuracy, rate, prosody, their combined scale score, and reading comprehension and cognitive load to determine the incremental influence of prosody on comprehension skills and general reading proficiency.\r\nEnhance the interpretability and usability of the project assessment for educators.\r\nBenefits\r\nCORE + Prosody Potential Benefits\r\nCORE + Prosody has the potential to increase the reliability and validity of decisions made from oral reading fluency assessment scores, resulting in better identification of students in need of reading interventions, and better evaluation of those interventions.\r\nThe CORE + Prosody assessment:\r\ncomputer-administered and -scored to reduce lost instructional time and administration costs\r\noffers a measure of prosody and a unified oral reading fluency score of accuracy, rate, and prosody to align with reading theory and provide educators with more meaningful reading information\r\nprovides a vertical scale across grades to improve score reliability and progress monitoring accuracy\r\nCORE\r\nComputerized Oral Reading Evaluation - CORE\r\nCORE + Prosody builds upon Computerized Oral Reading Evaluation (CORE), a project that uses (a) shorter passages, (b) automatic speech recognition to score oral reading fluency accuracy and rate, and (c) a latent variable psychometric model to scale, equate, and link scores across Grades 2 through 4 to improve reading outcomes for students across reading proficiency levels. Please visit the CORE project blog for information about the project procedures and results.\r\nCORE uses automatic speech recognition to score oral reading fluency accuracy and rate.3\r\nAlleviates the resource demands of one-to-one testing administration and the resource cost to train staff to administer and score oral reading fluency assessments\r\nReduces the time cost of oral reading fluency administration by allowing small-group or whole-classroom testing\r\nReduces human ORF administration errors by standardizing administration setting, delivery, and scoring\r\n\r\nCORE uses a latent variable psychometric model to scale, equate, and link scores across Grades 2 through 4.4\r\nThe model-based scores are on the same metric as traditional oral reading fluency scores: words correct per minute (WCPM). This makes the scale scores immediately usable for teachers and reading specialists who are familiar with the words correct per minute expectations for students at specific times in specific grades along the reading continuum.\r\nThe model-based words correct per minute scores offers educators scores that are comparable regardless of the passages read, and is a vast improvement on the common practice of using readability estimates (e.g., Flesch-Kincaid) to equate passages.\r\nCan mitigate the constraint of grade-level assessments by applying a common model-based words correct per minute scale across grades, providing an advantage for growth analyses of student oral reading fluency across grades.\r\nReduces standard error of oral reading fluency measurement 5, particularly for low-performing students, improving the reliability of scores and yielding scores sensitive to instructional change.\r\nThe model-based words correct per minute scores can increase the reliability and validity of the decisions made from scores, yielding better identification of students in need of reading interventions, and better evaluation of the results of those interventions.\r\n\r\nRelated\r\nDeveloping Computational Tools for Model-Based Oral Reading Fluency Assessments\r\nDeveloping Computational Tools for Model-Based Oral Reading Fluency Assessments, led by Principal Investigator Akihito Kamata.\r\nThis project expands upon the existing estimation model developed as part of the CORE project to include the development of (a) a sentence-level model that takes into account between-sentence dependency, and (b) incomplete reading. The model parameter estimation algorithms for these extensions will be developed by the method of moments, the Monte Carlo EM algorithm approach, and the Bayesian HMC approach used in the software package Stan.\r\nThis project will produce a Shiny app for rendering user-friendly R code needed to estimate model parameters, providing better oral reading fluency score comparability both within- and between-students for better longitudinal and cross-sectional studies, as well as better estimates of measurement errors for oral reading fluency scores. The research team will demonstrate the use of the new software on data collected by the CORE project, and develop web-based tutorials for supporting applied researchers who want to use the Shiny app.\r\n\r\n\r\n\r\n\r\n\r\nNational Reading Panel↩︎\r\nBenjamin & Schwanenflugel (2010); Miller & Schwanenflugel (2006); Valencia et al., (2010); Benjamin et al., (2013)↩︎\r\nNese & Kamata (2020)↩︎\r\nKara, Kamata, Potgieter, & Nese (2020)↩︎\r\nNese & Kamata (2020)↩︎\r\n",
      "last_modified": "2021-11-30T10:44:54-08:00"
    },
    {
      "path": "machine_prosody_scoring.html",
      "title": "Machine-Rated Prosody Study",
      "author": [
        {
          "name": "Zhongjie Wi",
          "url": {}
        },
        {
          "name": "George Sammit",
          "url": {}
        },
        {
          "name": "Joseph F. T. Nese",
          "url": {}
        },
        {
          "name": "Akihito Kamata",
          "url": {}
        },
        {
          "name": "Eric Larson",
          "url": {}
        }
      ],
      "date": "March 2022",
      "contents": "\r\n\r\nContents\r\nRelated Works\r\nDataset\r\nModelng\r\nFunctional\r\nLow Level Descriptor Convolutional Processing\r\nCNN Baseline\r\nOriginal X-vector\r\nWeighted X-vector\r\nSelf-Attention only\r\nSelf-Attention X-vector:\r\n\r\n\r\nResult Comparison\r\nFunctional Baseline Classification\r\nLLDs\r\n\r\n\r\n\r\n\r\nOther Activities Related to Prosody Analysis, Future Work\r\n\r\nDiscussion\r\n\r\nWe divide our discussion of progress in the past year into various sections. First, we introduce related work that guided many of our proposed architectural modeling attempts and dataset processing. Based on this literature review, we introduce modeling technologies that we believe enhance the performance of the model via leveraging advances in the field of machine learning (specifically, in convolutional networks applied to speech). We also investigate methods for comparing thee advances in a combined implementation of convolutional and traditional processing techniques. Finally, we conclude with a discussion of other efforts, challenges, and future work. A subset of the efforts described have also been selected for publication in ICASSP 2022, a premiere venue for work in speech processing.\r\n\r\nGeorge Sammit, Zhongjie Wu, Yihao Wang, Zhongdi Wu,* Akihito Kamata, Joe Nese, and Eric C. Larson (2022). Automated Prosody Classification for Oral Reading Fluency with Quadratic Kappa Loss and Attentive X-vectors. International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2022).\r\n\r\nRelated Works\r\nProject LISTEN, a reading tutor 1, began in the 1990s and is regarded as seminal research on automated analysis of children’s spoken reading. Much research in this field builds upon it. Ananthakrishnan and Narayanan 2 propose augmenting automatic speech recognizers (ASR) by adding symbolic alphabet annotations of prosodic events. Mostow and Duong 3 and then Duong, et al. 4 compared a child’s oral reading to that of an adult (of the same text) by analyzing the contours in pitch, intensity, pauses, and word reading times. In terms of scale, the closest to ours is the work of Sitaram and Mostow 5 (which builds on Duong & Mostow (2010) 6) who mined Project LISTEN’s database to evaluate oral prosody in an effort to predict fluency and comprehension. Bolaños et al. 7 8 combine lexical and prosodic features to analyze children’s oral reading based on the NAEP rating scale 9, a more standard and recognized scale than earlier studies. The most recent work in this area is from Sabu and Rao 10. They build a reading tutor for identifying lexical and prosodic miscues during oral reading similar to those identified by a trained professional.\r\nA number of works have investigated prosody correctness classification in this domain. Bolaños et al. (2013) \\(^{7\\;8}\\), using a similar rating scale, achieved lexical accuracy of 73.24% and prosodic accuracy of 69.73% when compared with human ratings. Aside from surpassing this benchmark, we introduce a number of novel concepts.\r\nWe use the concept of inter-rater reliability in our loss function 11 and leverage a number of concepts from X-vectors 12 and attentive X-vectors 13.\r\nWe employ weighted temporal pooling in our convolutional networks.\r\nWe summarize our contributions as follows:\r\nWe collected and validated a dataset of prosody correctness classification of 5,841 phrases collected from 1,335 students in 2nd through 4th grade.\r\nWe introduce the concept of average difference feature extraction, whereby the “ideal” prosody of a known phrase is estimated using text-to-speech (TTS) models.\r\nWe evaluate the use of weighted temporal pooling (inspired by X-Vectors \\(^{13}\\)) and weighted Kappa loss \\(^{11}\\) in prosody correctness classification.\r\nWe conduct an ablation study to investigate the overall importance of each processing procedure.\r\nOur work not only builds upon the aforementioned works, but also differs in several respects. First, our goal is solely to assess prosody according to a novel 4-scale rubric using conventional neural networks. Similar to many of these studies, our automated assessment is compared against expert judgement. However, we incorporate the disagreement of those judges into our model. Our sample size is considerably larger than previous studies both in terms of recordings and participants. While previous studies have informed our feature selection, we concentrate on both functional and prosodic low-level descriptors (LLDs) in the audio signal.\r\nDataset\r\nA full description of the procedures for the data can be found here\r\nIn order to design and evaluate our prosody classification algorithm, we collected audio samples of oral readings from a variety of schools in the Pacific Northwest region of the USA. (See here for study procedures.) Passages were written by an expert who also co-wrote the original easyCBM oral reading fluency 14 and reading comprehension passages 15. Each passage is an original work of fiction, and within 5 words of a targeted length: long = 85 words or medium = 50 words. Each passage has a beginning, middle, and end, follows either a “problem/resolution” or “sequence of events” format, and contains minimal use of dialogue and symbols. Exclusion rules for what could not appear in passages included: religious themes; trademark names, places, products; cultural/ethnic depictions; age-inappropriate themes (e.g., violence, guns, tobacco, drugs). All final CORE passages were reviewed by two experts in assessment for screening and progress monitoring for errors (e.g., format and grammatical), and bias (e.g., gender, cultural, religious, geographical). Final passages included 150 total passages, 50 at each of Grades 2-4, with 20 long passages (80-90 words), and 30 medium passages (45-55 words) for each grade.\r\nAlthough NAEP (2002) applied the scoring rubric to Grade 4, our research team made the decision to use the study-generated rubric and grade-calibrated passages which focus on phrasing, adherence to the author’s syntax, and expressiveness to assess prosody across Grades 2 through 4. In total, 49 audio samples were identified and scored by the research team as exemplars and used for training annotators and for certification of raters. A total of 63 human prosody raters were recruited and completed two training sessions, meeting the prosody certification. Raters score prosody on a four-point prosody scale, with the option to score between if they were not certain (1, 1.5, 2, …), thus creating a seven-point scale. Independent scores were averaged before taking the floor to provide the final score. Recording was reviewed by 138 groups of raters paired randomly in batches of approximately50. Each rater scored between 38 and 747 recordings (mean 217.7, SD 177.3). Initial analysis of the ratings showed inter-rater agreement of 95.8% within 1.0 point of disagreement (42.5% 0.0; 31.2% 0.5; 22.1% 1.0). Disagreement of larger than 1.0 was deemed abnormal by the expert author and held-out for re-review (255 samples). In this manner, 5,841 audio recordings, each scored by two raters, were made available for model training. Given the inherent ordinal nature of these classifications, intra-class correlation (ICC) 16 17 was used to validate acceptability of rater agreement. An acceptable mean of 0.74 (SD = 0.12) on the 7-point scale and 0.71 (SD = 0.12) on the final 4-point scale was observed.\r\nIn order to enhance model training result, we balanced amount of data with different scores using data augmentation. Training samples were augmented by:\r\nadding Gaussian noise\r\nadjusting gain\r\napplying a high/low/band-pass filter using the audiomentations Python library\r\nAugmentation was applied randomly over a range of the adjustments taking care not to distort the student’s voice. Less frequent classes were over-sampled to ensure balanced classes. In total, 4,128 new samples were added to functional training data (100% increase) and 14,092 new samples were added to LDD training data (240% increase).\r\nFrom these samples, we used two kinds of features for model training. Functional, which are summary data of the entire recording, were extracted with the 2016 Computational Paralinguistics Challenge (CompArE, 2016) 18 19. And low-level descriptors (LLDs), which are individual features taken every 100th of a second, were extracted using both the Extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) 20 and the CompArE. Model without augmented functional data is used during the first part of training process to give out a clear overview of augmented effect. Of all LLDs available, training is first done with sub-sections data and then combined into the full result. We obtain three sub-sections together that made up full LLDs data. We believe the following to be the most relevant to prosody classification, based on our literature survey.\r\nFunctional(CompArE)\r\nQuartile\r\nInter-quartile range\r\nPercentile\r\nPosition of min and max\r\nRelative duration LLD related\r\nMean, max, mean, and standard deviation of segment length\r\nGain of linear prediction (LP)\r\nCoefficient\r\neGeMaps\r\nFrequency-related (voicing-related) time series features\r\nLogarithmic F0 on a semitone frequency scale, starting at 27.5 Hz (semitone 0)\r\nCentre frequency and bandwidth of first, second, third formant\r\n\r\nEnergy-related frequency features, as they tend to be prosodic in nature (each of these features can be processed with a classical measure or via a convolutional filter)\r\nEstimate of perceived signal intensity from an auditory spectrum\r\n\r\nSpectral-related pitch features, as we expect them to allow for consistent comparisons (these features can be captured from the mel-scaled spectrogram using one dimensional convolution (across time))\r\nLinear regression slope of the logarithmic power spectrum within the band\r\nMel-Frequency Cepstral Coefficients\r\nDifference of the spectra of two consecutive frames\r\nFormant 1, 2, and 3 relative energies, as well as the ratio of the energy of the spectral harmonic peak at the first, second, third formant’s center frequency to the energy of the spectral peak at F0\r\nRatio of energy of the first F0 harmonic (H1) to the energy of the second F0 harmonic (H2) and the highest harmonic in the third formant range (A3)\r\n\r\nCompArE\r\nEnergy-related time series features, as they tend to be prosodic in nature (each of these features can be processed with a classical measure or via a convolutional filter)\r\nSum of auditory spectrum (loudness)\r\nSum of RASTA-style filtered auditory spectrum\r\nRMS energy, zero-crossing rate\r\n\r\nSpectral-related frequency features as we expect them to allow for consistent comparisons (these features can be captured from the mel-scaled spectrogram using one dimensional convolution (across time))\r\nRASTA-style auditory spectrum, bands 1–26 (0–8kHz)\r\nMel-frequency cepstral coefficients (MFCCs)\r\nSpectral energy 250–650 Hz, 1 k–4 kkHz\r\nSpectral flux, centroid, entropy, slope\r\nPsychoacoustic sharpness, harmonicity\r\nSpectral variance, skewness, kurtosis\r\n\r\nVoicing-related (F0) pitch features\r\nsubharmonic summation (SHS)\r\nViterbi smoothed scaling (across consonants, fricatives, and plosives)\r\n\r\nNote that, for CompArE, we also have the delta (defined as \\(LDD⁄\\Delta LDD\\)) of the above features to capture not only the magnitudes but also the relative changes. In addition, the following functions may be applied to the above LLD.\r\nMean value of peaks\r\nMean value of peaks – arithmetic mean\r\nMean/SD of inter-peak distances\r\nAmplitude means of peaks, of minima\r\nAmplitude range of peaks\r\nMean/SD of rising/falling slopes\r\nLinear regression slope, offset, quadratic error\r\nModelng\r\nWe use above pre-processed dataset as training data for modeling. To analyze each model, the result must be compared to human scoring results through a loss function. Most works use the Categorical Cross Entropy (CCE) as a loss function, which is a classic loss function for many classification tasks. However, due to the nature of prosody scoring, classes have a quantitative relation between each other that CCE cannot represent. Therefore, we also conducted experiments with a modified version of loss function: Quadratic Weighted Kappa (QWK) \\(^{11}\\).\r\nQWK loss is related to the calculation of Inter-Rater Reliability (IRR) that is typically measured between two human raters. QWK quantifies the seriousness of the disagreement between human rating and model output as:\r\n\\[\r\n\\kappa = 1 - \\frac{\\Sigma_{i, j} \\; \\omega_{i, j} \\; O_{i, j}}{\\Sigma_{i, j} \\; \\omega_{i, j} \\; E_{i, j}}\r\n\\] Where \\(O\\), \\(\\omega\\), and \\(E\\) are the confusion matrix, penalty weights matrix, and outer product of histogram of raters, respectively.\r\n\\(O\\), the confusion matrix, corresponds to the number of answers that receive a score \\(i\\) by the first rater and a score \\(j\\) by the second rater.\r\nA quadratic penalty weight matrix can be expressed as:\r\n\\[\r\n\\omega_{i, j} = \\frac{(i - j)^2}{(N - 1)^2}\r\n\\] where \\(N\\) is the total number of possible classes. Matrix \\(O\\) and matrix \\(E\\) are normalized to sum to 1.\r\nWhen optimizing with QWK, de La Torre et al. (2018) showed that the problem can be reformed as a minimization problem of \\(L\\) by:\r\n\\[\r\nL = log(1 - \\kappa + \\epsilon)\r\n\\] where \\(L \\in (-\\infty, log(2))\\) since \\(\\kappa \\in [-1, 1]\\), the log serves to decouple the numerator and denominator calculations, which in turn eases the computation of the gradient \\(^{11}\\). The \\(\\in\\) is a small value that avoids calculating \\(log(0)\\) for the loss function. In our dataset, we use the multiple scores from multiple raters as ground truth. When there is disagreement among raters, the QWK allows the loss to consider this disagreement. We hypothesize that such behavior is advantageous for prosody classification.\r\nWe use AdamW optimizer 21 to modify the learning rate.\r\nFunctional\r\nWe employ a baseline network (Figure 1) that uses residual connections with dense layer.\r\n\r\n\r\n\r\nFigure 1: Baseline network model for functional feature.\r\n\r\n\r\n\r\nA cosine learning rate schedule 22 is applied to adjust the running rate during the training process. Both augmented data and not augmented data is used during the training. This baseline network configuration is a common form of processing for features that are in a consistently sized vector (such as a table data). This network processes the fixed size vectors from the features generated. That is, this network is capable of processing features that have been aggregated per utterance. However, it cannot process the features over time before aggregation. For this style of processing, a convolutional network is required.\r\nLow Level Descriptor Convolutional Processing\r\nConvolutional networks (for audio) are neural networks that apply filters across the time series, extracting patterns in the audio streams over fixed time intervals. These filters are “learned” filters—meaning they are updated in the optimization process of the neural network. Because the filters are optimized, the network is optimized to extract the beast sets of features from the time series. This process was previously done with expert supervision of the features extracted—a time consuming process. Thus the use of convolutional layers allows a neural network to learn to identify patterns of interest and then classify the outputs of these patterns. One downside of these networks, however, is that convolutional filters must process fixed length inputs. That is, all audio files must be the same length—that is until the use of X-vectors \\(^{13}\\).\r\nIn our design, we borrow concepts from X-Vectors \\(^{12}\\) and attentive X-Vectors \\(^{13}\\) for use in our architectures. These original works used convolutional networks with adaptive statistics pooling layers. These adaptive pooling layers collapse the convolutional output filters over time using simple statistics such as mean and standard deviation. Thus, regardless of the length of the input, it can be processed by filters and collapsed to a fixed length vector. This means the same networks can be trained on audio of nearly any length. Intuitively, these network learn features of the audio, and then use statistics to characterize how these features change (or are distributed) over time. These characterizations are then used to classify how the feature changes can be mapped to the classification task of interest. In our case, we seek to map the changes to meaningful classification of Prosody. The original X-vectors architecture was employed for speaker authentication—learning how the feature changes can characterize a person’s voice. From that perspective, the convolutions learned by our architecture are quite different than the original X-vector architecture.\r\nIn our work, we do not employ the time-context layers in our work—we only build variations of temporal pooling methods that weight the convolution output activations before mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\)) are calculated into an embedding. We hypothesize the use of different temporal weighting schemes can help the model learn to ignore insignificant phrasal breaks in the spoken passage, while emphasizing other more costly prosodic mistakes. Several model architectures are constructed and investigated, described in the following sections.\r\nCNN Baseline\r\nWe employ a baseline network that uses residual connections with identity mappings \\(^{20}\\) and 1D time convolution, as shown in Figure 2. Each time convolutional block uses a number of separable and strided convolutions for downsampling. This baseline network does not use any temporal pooling before entering the output flow of the network. That is, the pooling layer is replaced by a flattening operation. This network does not use the statistics pooling from X-vectors and is therefore dubbed a “baseline” because it uses traditional processing.\r\n\r\n\r\n\r\nFigure 2: Baseline network model for ComPArE and eGeMaps LLD features.\r\n\r\n\r\n\r\nOriginal X-vector\r\nWe employ an X-vector-based network that uses traditional \\(\\mu\\) and \\(\\sigma^2\\) pooling of the convolutional output activations over time. Note that there is no silence detection employed as pauses are critical for the classification of prosody.\r\n\r\n\r\n\r\nFigure 3: Network model with Original X-vector for ComPArE and eGeMaps LLD features.\r\n\r\n\r\n\r\nWeighted X-vector\r\nWe employ a novel weighted x-vector architecture that uses the processing gate blocks to multiply the \\(\\mu\\) and \\(\\sigma^2\\) before pooling. This weighting is achieved through multiple 1D convolutions followed by a softmax layer to force the network to focus on certain time segments before entering the output flow. The weights are learned by the network and multiplied through the extracted features in the network. This has the effect of allowing the network to focus on certain areas over time, while ignoring other time segments. We hypothesized that this weighting would allow the network to learn feature variations that are critical for prosody, while ignoring other pauses and sentence structures that are not critical. A potential downside of this approach is that the weighting scheme requires the network to learn many more parameters. Thus, the optimization may be more difficult or unstable. To investigate this, we also propose a number of other weighting schemes that can be learned with fewer parameters, but can also be less expressive in how they tune the network.\r\n\r\n\r\n\r\nFigure 4: Network model with Weighted X-vector for ComPArE and eGeMaps LLD features.\r\n\r\n\r\n\r\nSelf-Attention only\r\nWe employ a network using convolutional self-attention instead of temporal pooling as shown in the Figure 5. Self-attention is similar to the weighted X-vector architecture except that the weighting layer is derived from the same processing branch as the input stream, rather than a separate branch. In this way, weights can still be used to weight time segments differently, but it is assumed the features used for classification are also the same features needed for weight classification. Thus, this network may not converge because the features in the feed forward branch are not sufficient for both tasks.\r\n\r\n\r\n\r\nFigure 5: Network model with self-attention weight for ComPArE and eGeMaps LLD features.\r\n\r\n\r\n\r\nSelf-Attention X-vector:\r\nFinally, we employ a self-attention weighted x-vector architecture that uses portions of both self attention and weighted X-vectors (Figure 6). Here self-attention is used to calculate the weighted vector to weight the segment before pooling, rather than the gate process blocks in method above.\r\n\r\n\r\n\r\nFigure 6: Network model with self-attention weighted X-vector for ComPArE and eGeMaps LLD features.\r\n\r\n\r\n\r\nResult Comparison\r\nA summary of the performance of each architecture on each dataset is presented in Table 1 below where each row represents a separate trained model (Functional with baseline only), and each “loss” column indicates whether the model was trained using CCE loss or QWK loss (\\(\\kappa\\)-loss). The leftmost “results” column shows performance using in-domain phrases – that is, known phrases are used in the training set. The rightmost column shows results across-domain phases where phrases are used that do not exist in the training set. In both scenarios, the training and testing sets are separated according to students (as previously described) such that a student is never in both the training and testing sets. For performance, the overall accuracy is shown for classifying prosody into four scales, as well as inter-rater reliability (IRR, linear \\(\\kappa\\)) assuming that the model is another prosody rater. We use the average of human raters as a ground truth for accuracy and IRR. The best performing models per-domain employ self-attention only and weighting with self-attention, respectfully.\r\n\r\n\r\nTable 1: Functional: Full-Delta Not Augmented\r\n\r\n\r\n\r\n\r\n\r\nIn-Domain\r\n\r\n\r\n\r\n\r\nCross-Domain\r\n\r\n\r\n\r\nLoss\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n60.3%\r\n\r\n\r\n0.52\r\n\r\n\r\n–\r\n\r\n\r\n–\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n4.0% (failed)\r\n\r\n\r\n0.00 (failed)\r\n\r\n\r\n–\r\n\r\n\r\n–\r\n\r\n\r\nFunctional Baseline Classification\r\nFunctional models are trained with functional dataset and with baseline model only. Table 2 below shows the result of training on not augmented and augmented dataset. The augmented data gives better performance on both CCE loss and QWK loss which failed during not augmented training. The best performance lies in cross-domain with CCE loss with 89.3% accuracy and 0.87 IRR. Also, functional model shows a relatively better performance in cross-domain result at about 62.8% accuracy and 0.53 IRR, which is the best cross-domain performance in all of the models. Because of the relatively poor cross domain performance, we conclude that the models learned should be passage specific. Cross domain classification is still an open research problem.\r\n\r\n\r\nTable 2: Functional: Full-Delta Augmented\r\n\r\n\r\n\r\n\r\n\r\nIn-Domain\r\n\r\n\r\n\r\n\r\nCross-Domain\r\n\r\n\r\n\r\nLoss\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n89.3%\r\n\r\n\r\n0.97\r\n\r\n\r\n62.8%\r\n\r\n\r\n0.53\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n68.7%\r\n\r\n\r\n0.63\r\n\r\n\r\n55.2%\r\n\r\n\r\n0.44\r\n\r\n\r\nLLDs\r\nFor LLD models, eGeMaps voicing gives out the best performance in in-domain using weighted X-vector with CCE loss with 90.4% accuracy and 0.90 IRR, which is also the best in-domain performance between all models, and a relatively good performance in cross-domain part using ComPArE voicing with self-attention weighted X-vector with Kappa loss at 58.5% accuracy. For best IRR in cross-domain, original X-vector with QWK performs better at 0.46 IRR. Full eGeMaps and ComPArE does not fit well with the new model, with the best performance in eGeMaps full baseline only at 87.2% accuracy and 0.85 IRR. The ComPArE Full models were never able to fully converge and thus we cannot conclude from their stated performances. They are still under adjustment on both learning rate algorithm and model itself. Meanwhile, we found that in cross-domain area, QWK loss performs better in both accuracy and IRR, even if the accuracy and IRR is still far from satisfaction. This might bring some of the idea for future studies.\r\n\r\n\r\n\r\n\r\nTable 3: ComPArE Voicing Model\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn-Domain\r\n\r\n\r\n\r\n\r\nCross-Domain\r\n\r\n\r\n\r\nClassifier\r\n\r\n\r\nLoss\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nBaseline\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n80.6%\r\n\r\n\r\n0.76\r\n\r\n\r\n45.8%\r\n\r\n\r\n0.27\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n68.7%\r\n\r\n\r\n0.69\r\n\r\n\r\n47.7%\r\n\r\n\r\n0.36\r\n\r\n\r\nX-Vector\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n82.8%\r\n\r\n\r\n0.80\r\n\r\n\r\n50.1%\r\n\r\n\r\n0.34\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n81.5%\r\n\r\n\r\n0.79\r\n\r\n\r\n48.0%\r\n\r\n\r\n0.31\r\n\r\n\r\nWeighted X-Vector\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n75.9%\r\n\r\n\r\n0.70\r\n\r\n\r\n46.6%\r\n\r\n\r\n0.27\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n80.4%\r\n\r\n\r\n0.78\r\n\r\n\r\n56.4%\r\n\r\n\r\n0.42\r\n\r\n\r\nX-Vector + Self-Attention\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n86.4%\r\n\r\n\r\n0.84\r\n\r\n\r\n52.6%\r\n\r\n\r\n0.39\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n74.9%\r\n\r\n\r\n0.73\r\n\r\n\r\n57.2%\r\n\r\n\r\n0.44\r\n\r\n\r\nWeighted X-Vector + Self-Attention\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n77.9%\r\n\r\n\r\n0.73\r\n\r\n\r\n48.5%\r\n\r\n\r\n0.30\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n79.5%\r\n\r\n\r\n0.77\r\n\r\n\r\n60.2%\r\n\r\n\r\n0.46\r\n\r\n\r\n\r\n\r\n\r\n\r\nTable 4: eGeMaps Voicing Model\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn-Domain\r\n\r\n\r\n\r\n\r\nCross-Domain\r\n\r\n\r\n\r\nClassifier\r\n\r\n\r\nLoss\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nBaseline\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n85.4%\r\n\r\n\r\n0.85\r\n\r\n\r\n51.3%\r\n\r\n\r\n0.33\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n69.5%\r\n\r\n\r\n0.66\r\n\r\n\r\n54.4%\r\n\r\n\r\n0.37\r\n\r\n\r\nX-Vector\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n75.7%\r\n\r\n\r\n0.74\r\n\r\n\r\n50.1%\r\n\r\n\r\n0.32\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n71.7%\r\n\r\n\r\n0.72\r\n\r\n\r\n57.1%\r\n\r\n\r\n0.46\r\n\r\n\r\nWeighted X-Vector\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n90.4%\r\n\r\n\r\n0.90\r\n\r\n\r\n50.5%\r\n\r\n\r\n0.32\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n73.2%\r\n\r\n\r\n0.75\r\n\r\n\r\n54.5%\r\n\r\n\r\n0.42\r\n\r\n\r\nX-Vector + Self-Attention\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n62.3%\r\n\r\n\r\n0.57\r\n\r\n\r\n47.0%\r\n\r\n\r\n0.26\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n73.0%\r\n\r\n\r\n0.73\r\n\r\n\r\n58.5%\r\n\r\n\r\n0.43\r\n\r\n\r\nWeighted X-Vector + Self-Attention\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n89.3%\r\n\r\n\r\n0.89\r\n\r\n\r\n52.0%\r\n\r\n\r\n0.34\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n72.1%\r\n\r\n\r\n0.74\r\n\r\n\r\n55.4%\r\n\r\n\r\n0.40\r\n\r\n\r\n\r\n\r\n\r\n\r\nTable 5: ComPArE Full Model\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn-Domain\r\n\r\n\r\n\r\n\r\nCross-Domain\r\n\r\n\r\n\r\nClassifier\r\n\r\n\r\nLoss\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nBaseline\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n37.7%\r\n\r\n\r\n–\r\n\r\n\r\n32.9%\r\n\r\n\r\n–\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n36.3%\r\n\r\n\r\n–\r\n\r\n\r\n36.1%\r\n\r\n\r\n–\r\n\r\n\r\nX-Vector\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n37.2%\r\n\r\n\r\n–\r\n\r\n\r\n32.6%\r\n\r\n\r\n–\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n39.3%\r\n\r\n\r\n–\r\n\r\n\r\n38.0%\r\n\r\n\r\n–\r\n\r\n\r\nWeighted X-Vector\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n37.3%\r\n\r\n\r\n–\r\n\r\n\r\n30.1%\r\n\r\n\r\n–\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n21.9%\r\n\r\n\r\n–\r\n\r\n\r\n22.0%\r\n\r\n\r\n–\r\n\r\n\r\nX-Vector + Self-Attention\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n35.9%\r\n\r\n\r\n–\r\n\r\n\r\n29.8%\r\n\r\n\r\n–\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n33.1%\r\n\r\n\r\n–\r\n\r\n\r\n34.2%\r\n\r\n\r\n–\r\n\r\n\r\nWeighted X-Vector + Self-Attention\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n35.6%\r\n\r\n\r\n–\r\n\r\n\r\n33.7%\r\n\r\n\r\n–\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n33.6%\r\n\r\n\r\n–\r\n\r\n\r\n33.1%\r\n\r\n\r\n–\r\n\r\n\r\n\r\n\r\n\r\n\r\nTable 6: eGeMaps Full Model\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn-Domain\r\n\r\n\r\n\r\n\r\nCross-Domain\r\n\r\n\r\n\r\nClassifier\r\n\r\n\r\nLoss\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nAccuracy\r\n\r\n\r\nIRR\r\n\r\n\r\nBaseline\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n87.2%\r\n\r\n\r\n0.85\r\n\r\n\r\n46.1%\r\n\r\n\r\n0.26\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n62.7%\r\n\r\n\r\n0.60\r\n\r\n\r\n47.8%\r\n\r\n\r\n0.31\r\n\r\n\r\nX-Vector\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n81.1%\r\n\r\n\r\n0.77\r\n\r\n\r\n45.2%\r\n\r\n\r\n0.24\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n59.3%\r\n\r\n\r\n0.57\r\n\r\n\r\n48.7%\r\n\r\n\r\n0.35\r\n\r\n\r\nWeighted X-Vector\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n73.8%\r\n\r\n\r\n0.67\r\n\r\n\r\n43.4%\r\n\r\n\r\n0.24\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n72.6%\r\n\r\n\r\n0.70\r\n\r\n\r\n50.2%\r\n\r\n\r\n0.32\r\n\r\n\r\nX-Vector + Self-Attention\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n79.8%\r\n\r\n\r\n0.75\r\n\r\n\r\n45.2%\r\n\r\n\r\n0.24\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n56.3%\r\n\r\n\r\n0.52\r\n\r\n\r\n48.9%\r\n\r\n\r\n0.35\r\n\r\n\r\nWeighted X-Vector + Self-Attention\r\n\r\n\r\nCCE Loss\r\n\r\n\r\n80.9%\r\n\r\n\r\n0.75\r\n\r\n\r\n47.5%\r\n\r\n\r\n0.27\r\n\r\n\r\n\r\n\r\nKappa-Loss\r\n\r\n\r\n76.1%\r\n\r\n\r\n0.71\r\n\r\n\r\n49.9%\r\n\r\n\r\n0.33\r\n\r\n\r\nOther Activities Related to Prosody Analysis, Future Work\r\nShowing confusion Matrix based on training result to show result differ in scores. This allows us to have a more general view of model accuracy and provides intuitive comparison between augmented and not augmented training result.\r\nCategorizing training results based on grade, gender, and passage. We allow trace back for the model prediction result to its related information to see its performance on different data.\r\nAdding cross domain validation result using unseen passage dataset. Features from passages that are not used during training result is categorized as cross domain and is used as testing set to show the model result on cross-domain.\r\nPreprocess data into TFRecord to increase the reading speed and lower the memory usage for training.\r\nDiscussion\r\nWe organize the discussion of results by research questions.\r\nR1: Can automated prosody classification for oral reading fluency be applied within or across domains reliably? \r\nWithin domain, the models perform similarly to (in many cases better than) human raters. Therefore, we conclude their use reasonable in this context. However, when applied across domain, this performance drops considerably. Therefore, automated cross-domain performance is still an open research topic for the community.\r\nR2: Do X-vector architectures provide an advantage over baseline convolutional models for prosody classification?\r\nBased on the performances over baseline, we can conclude that the X-Vector architecture provides a significant advantage in prosody classification.\r\nR3: Do X-vector weighting methods provide a distinct benefit?\r\nHere the results are not as straightforward. Using attention has an advantage, but weighting does not seem to provide an advantage. Therefore, we conclude that the most significant method for performance is attention. We note that, when applying attention, the number of weights in the model is reduced which might influence performance due to the dataset size.\r\nR4: Does using κ -loss provide a distinct benefit over traditional cross entropy?\r\nFor most models, there is not a clear advantage, but a performance boost is observed in others, especially in cross-domain. In general, we encourage other members of the speech processing community to employ and evaluate the QWK loss when subjective scores are used.\r\nR5: Is it better to turn result into Kappa related format?\r\nIn our result we are comparing the single model output with average score of two raters, while Kappa is more suitable for multi-rater situation. We are trying to simulate the real rating situation for the model and see if it performs better with multi-output compare with the original scores given by rater.\r\nR6: Is features taken every 100th of a second enough for the audio?\r\nThis question asks if is better to increase the rate of LLDs taken per audio. In similar research such as Sabu and Rao (2018)\\(^{10}\\), the intensity and spectral tilt are calculated every 10 ms, which is more frequent compared to our dataset. Whether a different frequency for data selection will result in better performance is still under consideration.\r\n\r\n\r\nHauptmann et al. (1994)↩︎\r\nAnanthakrishnan & Narayanan (2009)↩︎\r\nMostow & Duong (2009)↩︎\r\nDuong, et al. (2011)↩︎\r\nSitaram & Mostow (2012)↩︎\r\nDuong, M., & Mostow, J. (2010). Adapting a Duration Synthesis Model to Rate Children’s Oral Reading Prosody. In Eleventh Annual Conference of the International Speech Communication Association.↩︎\r\nBolaños et al. (2013)↩︎\r\nBolaños et al. (2013)↩︎\r\nNAEP (2002)↩︎\r\nSabu & Rao (2018)↩︎\r\nde la Torre et al (2018)↩︎\r\nSnyder et al (2018)↩︎\r\nOkabe et al. (2018)↩︎\r\nAlonzo & Tindal (2007)↩︎\r\nAlonzo & Tindal (2007)↩︎\r\nHallgren (2012)↩︎\r\nGisev et al. (2013)↩︎\r\nSchuller et al. (2014)↩︎\r\nLoshchilov & Hutter (2017)↩︎\r\nEyben et al. (2016)↩︎\r\nLoshchilov & Hutter (2019)↩︎\r\nLoshchilov & Hutter (2017)↩︎\r\n",
      "last_modified": "2022-02-15T13:51:36-08:00"
    },
    {
      "path": "research_team.html",
      "title": "Research Team",
      "author": [],
      "contents": "\r\n\r\n\r\nJoseph F. T. Nese, Ph.D. Princial Investigator\r\n\r\n\r\nJoe is a Research Associate Professor at Behavioral Research and Teaching at the University of Oregon. He received his Ph.D. in school psychology from the University of Maryland, and his B.A from the University of California at Santa Barbara. His goal is to bridge assessment and intervention in a meaningful way; to provide access to reliable and relevant data, interpret student responsiveness to intervention, and offer instructional recommendations to teachers to increase student achievement. Joe was the Principal Investigator of CORE, a project to develop and validate a computerized assessment system of oral reading fluency that serves as the foundational work for CORE + Prosody.\r\nAs PI of CORE + Prosody, Joe is responsible for all aspects of the CORE + Prosody project. He serves as a data scientist for the project, and leads the Human-Rated Prosody and the Comprehension & Cognitive Load Studies.\r\n\r\n\r\nAkihito Kamata, Ph.D. Co-Principal Investigator\r\n\r\n\r\nAki is a Professor in the Department of Education Policy & Leadership and the Department of Psyschology at Southern Methodist University. His primary research interest is psychometrics and educational and psychological measurement, focusing on development and implementation of item-level test data analysis methodology through various modeling framework, including item response theory, multilevel modeling, and structural equation modeling. Aki is also the Principal Investigator of a project to Develop Computational Tools for Model-Based Oral Reading Fluency Assessments.\r\nAs Co-PI of CORE + Prosody, Aki shares responsibility for all aspects of the project, and leads all psychometric modeling, including the study to measure and score prosody, and the calibration, equating, linking study.\r\n\r\n\r\nRhonda Nese, Ph.D. Co-Principal Investigator\r\n\r\n\r\nRhonda is an Assistant Professor in the Department of Special Education and Clinical Sciences and a Principal Investigator within Educational and Community Supports at the UO. Her research involves equitable intervention delivery within a multi-tiered behavior support framework focused on preventative strategies for improving student outcomes. Rhonda is the PI on an IES grant to refine and test an intervention to reduce exclusionary discipline practices, improve student behavior and student-teacher relationships, and increase instructional time for students in secondary settings, and Co-PI on three additional IES grants.\r\nRhonda brings expertise in training educators on equitable and feasible instructional practices, systems change, and policy implementation to support their community of learners. As Co-PI of CORE + Prosody, Rhonda shares responsibility for all aspects of the project, and leads the Feasibility of Use study, ensuring CORE + Prosody is feasible, useful, and relevant for educators, and efficient and valuable for administrators.\r\n\r\n\r\nLeilani Sáez, Ph.D. Co-Principal Investigator\r\n\r\n\r\nLeilani is a Research Assistant Professor in the College of Education at the University of Oregon. She obtained her Ph.D. in Educational Psychology from the University of California, Riverside.She has been the Principal or Co-Principal Investigator of nearly $3 million dollars in federally funded projects, and more than $50,000 in internal funding, to support teachers’ assessment-guided decision-making for preventing reading difficulties and enhance children’s reading development.\r\nLeilani has extensive experience working in classroom settings, particularly in the area of literacy instruction, including teaching, observation, and quantitative research. As Co-PI of CORE + Prosody, Leilani shares responsibility for all aspects of the project, leads the development of the reading comprehension questions, and co-leads the Comprehension & Cognitive Load Studies.\r\n\r\n\r\nJulie Alonzo, Ph.D. Assessment Systems\r\n\r\n\r\nJulie earned her Ph.D. in Educational Leadership with a specialization in Learning Assessment / Systems Performance at the University of Oregon in 2007. She earned her B.A. in English from Carleton College in 1990, her NBPTS Certification in Adolescent and Young Adult English Language Arts, and worked as a high school teacher and part-time administrator for 12 years prior to her work at the UO. Julie is the Co-Director at Behavioral Research and Teaching, and the Director of the Doctorate of Education (D.Ed.) program at the UO. Her primary research interests include teacher and administrator professional development and the meaningful inclusion of students with diverse learning needs.\r\nWith expertise in curriculum-based measurement and teacher professional development, Julie provides guidance on the CORE + Prosody assessment development, and the prosody training for the Human-Rated Prosody study.\r\n\r\n\r\nEric Larson, Ph.D. Computer Engineering\r\n\r\n\r\nEric is an Associate Professor in Computer Science in the Bobby B. Lyle School of Engineering, Southern Methodist University. He is a fellow of the Hunt Institute for Engineering Humanity, a member of the Darwin Deason Institute for Cyber-security, and a member of the ATT center for virtualization. His research explores the interdisciplinary relationship of machine learning and signal/image processing with the fields of security, mobile health, education, psycho-visual psychology, human-computer interaction, and ubiquitous computing.\r\nAs the Computer Engineer for CORE + Prosody, Dr. Larson is responsible for developing the project’s prosody algorithm, ASR engine, and tablet application.\r\n\r\n\r\nCornelis Potgieter, Ph.D. Statistics Consultant\r\n\r\n\r\nCornelis received his Ph.D. in Statistics from the University of Johannesburg in 2009. He is currently an Assistant Professor in the Department of Mathematics at Texas Christian University, and also holds an appointment as Visiting Senior Research Associate in the Department of Statistics at the University of Johannesburg, South Africa. His primary research interests are nonparametric statistics and educational (psychometric) measurement, focusing on the development of new methodology in the areas of treatment effect estimation, measurement error & latent variable modeling, and capturing asymmetry in data using generalized skew-symmetric distributions.\r\nCornelis will consult with CORE + Prosody investigators to help derive the parameter estimation algorithm for the integrated psychometric model for prosody, accuracy, and rate parameters.\r\n\r\n\r\n\r\n\r\n\r\n\r\nXinyi Ding, Ph.D. Machine Learning\r\n\r\n\r\nXinyi is an Assistant Professor in the Department of Computer Science and Technology at ZheJiang GongShang University. He received his Ph.D. from Southern Methodist University in 2020. His research interests include ubiquitous computing, machine learning, and educational data mining.\r\n\r\n\r\nNoboru Matsuda, Ph.D. Artificial Intelligence Consultant\r\n\r\n\r\nNoboru is an Associate Professor of Computer Science and a Director of the Innovative Educational Computing Laboratory at North Carolina State University, whose research focus is on technology innovation and integration to of Artificial Intelligence technologies to advance the sciences of learning. Noboru was awarded an IES grant to develop an online learning environment to learn to solve liner algebra equations by teaching a synthetic peer (aka a teachable agent), called SimStudent, and an intervention called APLUS (Artificial Peer Learning environment Using SimStudent). His scholarly expertise spans education, learning science, cognitive science, and computer science.\r\nNoboru will consult with CORE + Prosody investigators on the machine learning aspects of the Automated Prosody Scoring Study (e.g., feature selection, convolutional network) and the automatic speech recognition (ASR) Development.\r\n\r\n\r\nGulcan Cil, Ph.D.Cost Analysis Consultant\r\n\r\n\r\nGulcan is a health economist with a particular focus on policy impact evaluation and identification of the effects of behavioral and environmental factors on health and other socio-economic outcomes. Her research expertise is in analyses of large data sets from administrative records using modern statistical techniques for improved causal inference. She has extensive experience in large-scale policy evaluation in terms of behavioral responses they induce and their impacts on individual outcomes. She has also participated in cost studies for school-based programs, and studies that involve population-level evaluation of school- or community-based programs.\r\nGulcan will consult with CORE + Prosody investigators on Cost Analysis, ensuring the adequate collection of resources needed for the “ingredients approach”, information tracking, and analysis consultation.\r\n\r\n\r\nAaron GlasgowTechnology Development\r\n\r\n\r\nAaron has extensive experience in software engineering with a focus on full-stack web application development for the K-12 environment. Aaron has served as the technical coordinator and lead developer on many federal grants and state contracts. Aaron routinely communicates with diverse audiences of varying technical background and helps to achieve functional outcomes. He has made significant contributions to the easyCBM assessment system and several alternate/extended assessment systems used to meet statewide k-12 accountability requirements.\r\nFor CORE + Prosody, Aaron is responsible for managing and developing the software for the Human-Rated Prosody study, ensuring the technical adequacy of data collection, and collaborating wih SMU on the tablet application development.\r\n\r\n\r\nMakayla Whitney Graduate Researcher\r\n\r\n\r\nMakayla is a master’s student in the department of Educational Methodology, Policy, and Leadership at the University of Oregon. Her thesis is focused on creating a reading assessment for illiterate adults that includes aspects of comprehension. She received her a bachelor’s degree from Humboldt State University in Anthropology with focuses in Linguistics and Teaching English as a Second Language (ESL). Her background is in adult literacy, ESL studies, adult assessments, and reading materials.\r\nFor CORE + Prosody, Makayla assisted with the development of the prosody rubric and worked as a prosody rater in the Human Rated Prosody Study.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-06-30T12:25:42-07:00"
    }
  ],
  "collections": []
}
